{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --quiet transformers datasets peft wandb accelerate trl","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:01:19.305599Z","iopub.execute_input":"2024-07-19T21:01:19.306208Z","iopub.status.idle":"2024-07-19T21:01:34.678549Z","shell.execute_reply.started":"2024-07-19T21:01:19.306174Z","shell.execute_reply":"2024-07-19T21:01:34.677218Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --quiet -i https://pypi.org/simple/ bitsandbytes\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:01:34.681237Z","iopub.execute_input":"2024-07-19T21:01:34.681693Z","iopub.status.idle":"2024-07-19T21:01:51.466636Z","shell.execute_reply.started":"2024-07-19T21:01:34.681649Z","shell.execute_reply":"2024-07-19T21:01:51.465485Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Set your Hugging Face token\nos.environ['HF_TOKEN'] = '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'\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:01:51.472315Z","iopub.execute_input":"2024-07-19T21:01:51.472904Z","iopub.status.idle":"2024-07-19T21:01:51.477766Z","shell.execute_reply.started":"2024-07-19T21:01:51.472863Z","shell.execute_reply":"2024-07-19T21:01:51.476832Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\n# import wandb","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:01:51.478887Z","iopub.execute_input":"2024-07-19T21:01:51.479223Z","iopub.status.idle":"2024-07-19T21:01:54.653020Z","shell.execute_reply.started":"2024-07-19T21:01:51.479191Z","shell.execute_reply":"2024-07-19T21:01:54.651573Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Run Model with the Adapter","metadata":{}},{"cell_type":"code","source":"# import wandb\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\nfrom datasets import load_dataset, DatasetDict\nfrom peft import get_peft_model, LoraConfig, PeftType\nimport torch\nimport accelerate","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:01:54.654261Z","iopub.execute_input":"2024-07-19T21:01:54.654662Z","iopub.status.idle":"2024-07-19T21:02:09.961871Z","shell.execute_reply.started":"2024-07-19T21:01:54.654636Z","shell.execute_reply":"2024-07-19T21:02:09.961061Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-19 21:01:57.847132: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-19 21:01:57.847278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-19 21:01:57.962657: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/outputs/final_model\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:03:33.035027Z","iopub.execute_input":"2024-07-19T21:03:33.035791Z","iopub.status.idle":"2024-07-19T21:03:33.042845Z","shell.execute_reply.started":"2024-07-19T21:03:33.035758Z","shell.execute_reply":"2024-07-19T21:03:33.041911Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['adapter_model.safetensors',\n 'adapter_config.json',\n 'training_args.bin',\n 'special_tokens_map.json',\n 'tokenizer.model',\n 'README.md',\n 'tokenizer.json',\n 'tokenizer_config.json']"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GemmaTokenizer\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:02:09.970751Z","iopub.execute_input":"2024-07-19T21:02:09.971058Z","iopub.status.idle":"2024-07-19T21:02:09.981191Z","shell.execute_reply.started":"2024-07-19T21:02:09.971032Z","shell.execute_reply":"2024-07-19T21:02:09.980464Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model_id = \"google/gemma-2b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, force_download=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,device_map='auto', force_download=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:02:11.665103Z","iopub.execute_input":"2024-07-19T21:02:11.665877Z","iopub.status.idle":"2024-07-19T21:02:36.207104Z","shell.execute_reply.started":"2024-07-19T21:02:11.665844Z","shell.execute_reply":"2024-07-19T21:02:36.206098Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a57127054515432cb35188d2b91b4d6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67cbb4cb51e3408785e9e1d15bc6b7b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c92b97b28d0f44b69a16f5ffb11fdd7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13dd0398a2c0421c93b78ee065df89fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de57e773b3c44c59008b49912b82d9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2d3b69f928a42ff9363e115b0954734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc899ed15aaf43cbb9631431b75b2ff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f64a5890594440f68ab9d6551a1ece2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e8c19656014427992b41c6e4d60bda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"193e8531932547ea8e3b402a1f91b82c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e12a0e3efe49d5925617d3e5917be0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf46f6a1991e4d5dab4e4863b5d8146d"}},"metadata":{}},{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1c94de4dc1545f8a7cc19371c1d2321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63b6f7dd1aa94ee9a5348b0b1d604122"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel\n\n# Path to your saved adapter files\nadapter_model_path = \"/kaggle/working/outputs/checkpoint-160\"\nadapter_config_path = \"/kaggle/working/outputs/checkpoint-160/adapter_config.json\"\n\n# Load the adapter\npeft_model = PeftModel.from_pretrained(\n    model,\n    adapter_model_path,\n    adapter_config_path=adapter_config_path\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:25:02.607268Z","iopub.execute_input":"2024-07-19T21:25:02.607640Z","iopub.status.idle":"2024-07-19T21:25:02.933600Z","shell.execute_reply.started":"2024-07-19T21:25:02.607613Z","shell.execute_reply":"2024-07-19T21:25:02.932523Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"peft_model","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:06:19.562456Z","iopub.execute_input":"2024-07-19T21:06:19.562813Z","iopub.status.idle":"2024-07-19T21:06:19.578694Z","shell.execute_reply.started":"2024-07-19T21:06:19.562786Z","shell.execute_reply":"2024-07-19T21:06:19.577746Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): GemmaForCausalLM(\n      (model): GemmaModel(\n        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n        (layers): ModuleList(\n          (0-17): 18 x GemmaDecoderLayer(\n            (self_attn): GemmaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): GemmaRotaryEmbedding()\n            )\n            (mlp): GemmaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=16384, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=16384, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=16384, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): GemmaRMSNorm()\n            (post_attention_layernorm): GemmaRMSNorm()\n          )\n        )\n        (norm): GemmaRMSNorm()\n      )\n      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"input_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = peft_model.generate(**input_ids, max_length=300)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:09:05.885828Z","iopub.execute_input":"2024-07-19T21:09:05.886594Z","iopub.status.idle":"2024-07-19T21:09:26.844989Z","shell.execute_reply.started":"2024-07-19T21:09:05.886557Z","shell.execute_reply":"2024-07-19T21:09:26.844060Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# wandb.init(project=\"gemma_finetune\", name=\"oasst2_finetuning\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"tuneai/oasst2_top1_chatgpt_format\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:10:08.615995Z","iopub.execute_input":"2024-07-19T21:10:08.616856Z","iopub.status.idle":"2024-07-19T21:10:11.953850Z","shell.execute_reply.started":"2024-07-19T21:10:08.616828Z","shell.execute_reply":"2024-07-19T21:10:11.952603Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/471 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84605c092e148748dcea426b9ed17f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/10.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48bb581dc4342fca05f6355cbc28a76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/10746 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68e8cd801ac141e4907130b4d6556003"}},"metadata":{}}]},{"cell_type":"code","source":"dataset['train']['conversations'][1]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:10:11.957168Z","iopub.execute_input":"2024-07-19T21:10:11.957483Z","iopub.status.idle":"2024-07-19T21:10:12.339770Z","shell.execute_reply.started":"2024-07-19T21:10:11.957455Z","shell.execute_reply":"2024-07-19T21:10:12.338712Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[{'from': 'human',\n  'value': 'I am making mayonnaise, it was starting to thicken but now it has become runny and liquid again, is there any way to salvage it?'},\n {'from': 'gpt',\n  'value': \"Yes, it's possible to fix runny mayonnaise! The most common reason for mayonnaise becoming runny is because the oil was added too quickly or the egg yolk wasn't emulsified properly. Here are some steps you can take to fix it:\\n\\n1. Separate another egg yolk and place it in a clean, dry bowl.\\n2. Slowly add the runny mayonnaise to the egg yolk while whisking vigorously.\\n3. Once all the runny mayonnaise has been added, continue whisking until the mixture has emulsified and thickened.\\n4. If the mayonnaise is still too runny, you can add another egg yolk and repeat the process.\\n\\nIf the mayonnaise still won't thicken, you can try adding a small amount of dijon mustard or vinegar to the mixture, which can act as emulsifiers and help stabilize the mayonnaise. It's important to add these ingredients slowly and in small amounts to avoid over-thinning the mixture.\"},\n {'from': 'human', 'value': 'What is optimal Mayonnaise thickness?'},\n {'from': 'gpt',\n  'value': 'The optimal mayonnaise thickness will depend on how it is being used. A runny mayonnaise may be good in chicken salad while a thicker mayonnaise may be better spread over a hamburger bun. The only way to determine your personal preference is to test different levels of viscosity in with different foods.'}]"},"metadata":{}}]},{"cell_type":"code","source":"# import wandb\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\nfrom datasets import load_dataset, DatasetDict\nfrom peft import get_peft_model, LoraConfig, PeftType\nimport torch\nimport accelerate","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:20:27.840264Z","iopub.execute_input":"2024-07-19T17:20:27.840815Z","iopub.status.idle":"2024-07-19T17:20:32.712728Z","shell.execute_reply.started":"2024-07-19T17:20:27.840782Z","shell.execute_reply":"2024-07-19T17:20:32.711725Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-07-19 17:20:29.448570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-19 17:20:29.448632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-19 17:20:29.450303: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GemmaTokenizer\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:20:35.473043Z","iopub.execute_input":"2024-07-19T17:20:35.474253Z","iopub.status.idle":"2024-07-19T17:20:35.481170Z","shell.execute_reply.started":"2024-07-19T17:20:35.474215Z","shell.execute_reply":"2024-07-19T17:20:35.480235Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model_id = \"google/gemma-2b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, force_download=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,device_map='auto', force_download=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:20:36.313728Z","iopub.execute_input":"2024-07-19T17:20:36.314412Z","iopub.status.idle":"2024-07-19T17:21:45.353184Z","shell.execute_reply.started":"2024-07-19T17:20:36.314377Z","shell.execute_reply":"2024-07-19T17:21:45.352182Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f985586facda4acfba842c0cd804ebe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4790c7c79d6941b99dc02401a2ee91c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27c151fbb79543b09f6cac2efcec8112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a0eae7d26d040a79fddaa717e922bda"}},"metadata":{}},{"name":"stderr","text":"Gemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b3ac31cd8214c8aa674753ad2b9ccdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"019eb6e6253f41e9bb2a85b9f5526888"}},"metadata":{}}]},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:22:37.495564Z","iopub.execute_input":"2024-07-19T17:22:37.495990Z","iopub.status.idle":"2024-07-19T17:22:37.791854Z","shell.execute_reply.started":"2024-07-19T17:22:37.495944Z","shell.execute_reply":"2024-07-19T17:22:37.790813Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"51"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = []\n    for conversation in examples[\"conversations\"]:\n        # Concatenate all messages in the conversation\n        conversation_text = \"\"\n        for message in conversation:\n            conversation_text += f\"{message['from']}: {message['value']} \"\n        inputs.append(conversation_text.strip())\n    \n    # Tokenize the concatenated conversation text\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:10:36.996262Z","iopub.execute_input":"2024-07-19T21:10:36.996631Z","iopub.status.idle":"2024-07-19T21:10:37.002481Z","shell.execute_reply.started":"2024-07-19T21:10:36.996603Z","shell.execute_reply":"2024-07-19T21:10:37.001551Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# # Apply the preprocessing function to the dataset\ntokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:10:41.867545Z","iopub.execute_input":"2024-07-19T21:10:41.868171Z","iopub.status.idle":"2024-07-19T21:10:49.985191Z","shell.execute_reply.started":"2024-07-19T21:10:41.868131Z","shell.execute_reply":"2024-07-19T21:10:49.984171Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10746 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82d342de250d4b4e9e2588e188459694"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# Split the tokenized dataset into train and validation sets (90-10 split)\ntrain_test_split = dataset[\"train\"].train_test_split(test_size=0.1)\ndataset_split = DatasetDict({\n    'train': train_test_split['train'],\n    'validation': train_test_split['test']\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:31:10.342364Z","iopub.execute_input":"2024-07-19T17:31:10.343341Z","iopub.status.idle":"2024-07-19T17:31:10.361942Z","shell.execute_reply.started":"2024-07-19T17:31:10.343295Z","shell.execute_reply":"2024-07-19T17:31:10.360985Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"eval_dataset = dataset_split['validation']\ntokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:32:18.946147Z","iopub.execute_input":"2024-07-19T17:32:18.946823Z","iopub.status.idle":"2024-07-19T17:32:20.289759Z","shell.execute_reply.started":"2024-07-19T17:32:18.946790Z","shell.execute_reply":"2024-07-19T17:32:20.288823Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1075 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d482b676dd432587f68ffcc71dec31"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_eval_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:32:31.704931Z","iopub.execute_input":"2024-07-19T17:32:31.705650Z","iopub.status.idle":"2024-07-19T17:32:31.711333Z","shell.execute_reply.started":"2024-07-19T17:32:31.705618Z","shell.execute_reply":"2024-07-19T17:32:31.710390Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['conversations', 'input_ids', 'attention_mask'],\n    num_rows: 1075\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Function to generate predictions\ndef generate_predictions(input_ids, attention_mask):\n    with torch.no_grad():\n        # Ensure tensors are on the same device as the model\n        input_ids = input_ids.to(model.device)\n        attention_mask = attention_mask.to(model.device)\n        \n        # Generate outputs\n        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=50)\n        # Decode the outputs\n        return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:33:08.974033Z","iopub.execute_input":"2024-07-19T17:33:08.974425Z","iopub.status.idle":"2024-07-19T17:33:08.980224Z","shell.execute_reply.started":"2024-07-19T17:33:08.974394Z","shell.execute_reply":"2024-07-19T17:33:08.979272Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Apply the prediction function\npredictions = []\ninputs = []\n# Iterate over the dataset in batches\nfor batch in tokenized_eval_dataset:\n    # Convert batch to tensors\n    conversations = batch['conversations']\n    input_ids = torch.tensor(batch['input_ids'])\n    attention_mask = torch.tensor(batch['attention_mask'])\n    inputs.extend(conversations)\n    # Ensure tensors are properly shaped\n    if input_ids.ndim == 1:  # If single instance, add batch dimension\n        input_ids = input_ids.unsqueeze(0)\n        attention_mask = attention_mask.unsqueeze(0)\n    \n    # Generate predictions for the batch\n    batch_predictions = generate_predictions(input_ids, attention_mask)\n    predictions.extend(batch_predictions)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:47:13.982158Z","iopub.execute_input":"2024-07-19T17:47:13.983015Z","iopub.status.idle":"2024-07-19T17:48:36.412307Z","shell.execute_reply.started":"2024-07-19T17:47:13.982978Z","shell.execute_reply":"2024-07-19T17:48:36.410791Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":37,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Generate predictions for the batch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(batch_predictions)\n","Cell \u001b[0;32mIn[31], line 9\u001b[0m, in \u001b[0;36mgenerate_predictions\u001b[0;34m(input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Generate outputs\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Decode the outputs\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2498\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:1121\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1118\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1135\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:926\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    915\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    916\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    917\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    923\u001b[0m         cache_position,\n\u001b[1;32m    924\u001b[0m     )\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 926\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:646\u001b[0m, in \u001b[0;36mGemmaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:543\u001b[0m, in \u001b[0;36mGemmaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    541\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    542\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 543\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    546\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:468\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    467\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 468\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:574\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatMul4Bit\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, quant_state)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemv_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/functional.py:1965\u001b[0m, in \u001b[0;36mgemv_4bit\u001b[0;34m(A, B, out, transposed_A, transposed_B, state)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgemv_4bit\u001b[39m(\n\u001b[1;32m   1958\u001b[0m     A: Tensor,\n\u001b[1;32m   1959\u001b[0m     B: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1963\u001b[0m     state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1964\u001b[0m ):\n\u001b[0;32m-> 1965\u001b[0m     prev_device \u001b[38;5;241m=\u001b[39m \u001b[43mpre_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;66;03m# sout = check_matmul(A, B, out, transposed_A, transposed_B, expected_type=A.dtype)\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/functional.py:459\u001b[0m, in \u001b[0;36mpre_call\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpre_call\u001b[39m(device):\n\u001b[1;32m    458\u001b[0m     prev_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()\n\u001b[0;32m--> 459\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prev_device\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:402\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_device\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the current device.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    Usage of this function is discouraged in favor of :any:`device`. In most\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m            if this argument is negative.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_get_device_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    404\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_setDevice(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/_utils.py:34\u001b[0m, in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a cuda or cpu device, but got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a cuda device, but got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"predictions[2]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:48:50.370074Z","iopub.execute_input":"2024-07-19T17:48:50.370452Z","iopub.status.idle":"2024-07-19T17:48:50.376666Z","shell.execute_reply.started":"2024-07-19T17:48:50.370422Z","shell.execute_reply":"2024-07-19T17:48:50.375758Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"'human: Could you write a list of ten short messages as though they were said by a medieval merchant? gpt: Sure. Here are ten short messages as though they were said by a medieval merchant:\\n\\n1. \"Aye, good morrow, my lord. Might I interest thee in some fine silk from the East?\"\\n2. \"Hark! The shipment of spices hath arrived. Come, see the treasures from distant lands!\"\\n3. \"Ho there, travelers! My shop offers the finest swords and armor in all the realm.\"\\n4. \"Fair lady, thou shalt find no better deals on fine jewelry than at my humble stall.\"\\n5. \"Pray, tell me, sir knight. Are you in need of a sturdy mount for thy journeys?\"\\n6. \"Good people! Come taste the sweetest fruits and wines from the vineyards of France!\"\\n7. \"By Saint George, the wool from my sheep is of the highest quality. Come, see for thyself!\"\\n8. \"Greetings, good folk. Allow me to offer ye the finest handcrafted pottery in all the land.\"\\n9. \"Hear ye, hear ye! The finest furs and pelts from the Northern forests are now available for purchase.\"\\n10. \"Ahoy there, sailors! My ship has returned from the distant lands of Cathay with exotic goods aplenty.\" human: Write these messages in Old High German. gpt: Sure, here are the ten short messages translateed into Old High German as though they were said by a medieval merchant:\\n\\n1. \"Wola, guoter morgin, min herro. Mag ich dich interressiern an schôner seide aus dem Ôsten?\"\\n2. \"Hôrt! Der schifft von spezereien ist angekomen. Kummt, schauet die schâtze von fernen lenden!\"\\n3. \"Hô! Reisende! Mein laden bittet die schônsten swerte und harnische im ganzen reich.\"\\n4. \"Schône frouwe, ihr findit kein bessers angebot für edele juwelen als an meinem bescheidenem stand.\"\\n5. \"Bitte, sagt mir, werter ritter. Brauchest du ein starker pferd für deine reisen?\"\\n6. \"Gute leute! Kommt und kostet die süßesten früchte und weine aus den weingärten von Frankreich!\"\\n7. \"Bei sancte Georgi, das woll aus meiner schaf ist von der höchsten quality. Kummt, schauet es dir selbst!\"\\n8. \"Ahoy there, sailors! My ship has returned from the distant lands of Cathay with exotic goods aplenty.\"\\n9. \"Ahoy there,'"},"metadata":{}}]},{"cell_type":"code","source":"inputs[2]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:49:01.167534Z","iopub.execute_input":"2024-07-19T17:49:01.167913Z","iopub.status.idle":"2024-07-19T17:49:01.174194Z","shell.execute_reply.started":"2024-07-19T17:49:01.167882Z","shell.execute_reply":"2024-07-19T17:49:01.173228Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"{'from': 'human', 'value': 'Danke für die ausführliche Erklärung'}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/outputs/checkpoint-160\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/outputs/checkpoint-160\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = []\n    for conversation in examples[\"conversations\"]:\n        # Concatenate all messages in the conversation\n        conversation_text = \"\"\n        for message in conversation:\n            conversation_text += f\"{message['from']}: {message['value']} \"\n        inputs.append(conversation_text.strip())\n    \n    # Tokenize the concatenated conversation text\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    return model_inputs\n\n# Apply the preprocessing function to the dataset\ntokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:11:12.949537Z","iopub.execute_input":"2024-07-19T21:11:12.950185Z","iopub.status.idle":"2024-07-19T21:11:21.022533Z","shell.execute_reply.started":"2024-07-19T21:11:12.950153Z","shell.execute_reply":"2024-07-19T21:11:21.021621Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10746 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf236b1bac244c9e8bbb9e80769e5ccc"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# Split the tokenized dataset into train and validation sets (90-10 split)\ntrain_test_split = tokenized_dataset[\"train\"].train_test_split(test_size=0.1)\ndataset_split = DatasetDict({\n    'train': train_test_split['train'],\n    'validation': train_test_split['test']\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:11:21.024302Z","iopub.execute_input":"2024-07-19T21:11:21.024608Z","iopub.status.idle":"2024-07-19T21:11:21.045977Z","shell.execute_reply.started":"2024-07-19T21:11:21.024582Z","shell.execute_reply":"2024-07-19T21:11:21.045183Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Print the number of samples in each split to verify\nprint(f\"Train dataset length: {len(dataset_split['train'])}\")\nprint(f\"Validation dataset length: {len(dataset_split['validation'])}\")\n\n# Verify a sample\nprint(dataset_split[\"train\"][0])\nprint(dataset_split[\"validation\"][0])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:11:21.047446Z","iopub.execute_input":"2024-07-19T21:11:21.047840Z","iopub.status.idle":"2024-07-19T21:11:21.173226Z","shell.execute_reply.started":"2024-07-19T21:11:21.047804Z","shell.execute_reply":"2024-07-19T21:11:21.172317Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Train dataset length: 9671\nValidation dataset length: 1075\n{'conversations': [{'from': 'human', 'value': 'Estoy pensando en comprarme una planta de interior.\\n¿Qué tipo de cosas tengo que revisar antes de comprármela?\\n¿Tendría que preocuparme de dónde ponerla dentro de casa (al la do de una ventana o un balcón o no)?'}, {'from': 'gpt', 'value': 'Antes de comprar una planta de interior, hay varios factores que debes considerar:\\n\\n1. Iluminación: Asegúrate de elegir una planta que requiera la cantidad de luz que se puede encontrar en el lugar donde la quieres poner. Algunas plantas necesitan mucha luz solar directa mientras que otras prefieren luz indirecta.\\n\\n2. Humedad: Considera la humedad del ambiente donde la quieres colocar. Algunas plantas necesitan una humedad alta mientras que otras pueden tolerar ambientes secos.\\n\\n3. Tamaño: Tienes en cuenta el tamaño de la planta en adultez, ya que algunas pueden crecer muy grandes y ocupar mucho espacio.\\n\\n4. Facilidad de cuidado: Algunas plantas son más fáciles de cuidar que otras, así que es importante elegir una que sea adecuada para tu nivel de experiencia y tiempo disponible.\\n\\nEn cuanto a la ubicación, sí es importante preocuparse por donde poner la planta dentro de casa. Un lugar cerca de una ventana o un balcón es ideal para la mayoría de las plantas, ya que les proporciona luz natural. Sin embargo, es importante tener en cuenta que la luz solar directa puede ser perjudicial para algunas plantas, así que asegúrate de elegir un lugar adecuado para la especie que hayas elegido.'}], 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 17877, 235292, 128668, 76213, 659, 21808, 504, 1749, 33207, 581, 9022, 235265, 108, 235737, 13499, 10095, 581, 19550, 32218, 907, 134283, 12284, 581, 942, 7069, 235255, 76237, 235336, 108, 235737, 141027, 30948, 907, 219453, 504, 581, 54284, 33679, 522, 15255, 581, 7467, 591, 492, 683, 749, 581, 1749, 60440, 493, 748, 163242, 493, 793, 15939, 583, 1071, 235292, 74496, 581, 21808, 1749, 33207, 581, 9022, 235269, 7271, 27050, 65338, 907, 59042, 77505, 235292, 109, 235274, 235265, 4626, 4863, 2275, 235292, 138760, 173396, 581, 50043, 1749, 33207, 907, 1409, 15477, 683, 27303, 581, 16601, 907, 699, 7492, 21861, 659, 822, 10609, 11226, 683, 47482, 33679, 235265, 129228, 37885, 86427, 35205, 16601, 12677, 77812, 20621, 907, 17227, 133632, 479, 16601, 7626, 132913, 235265, 109, 235284, 235265, 8891, 72123, 235292, 15320, 235250, 683, 105439, 1177, 26652, 11226, 683, 47482, 49828, 235265, 129228, 37885, 86427, 1749, 105439, 14671, 20621, 907, 17227, 13579, 15227, 486, 104230, 136247, 235265, 109, 235304, 235265, 116586, 235292, 222637, 659, 13691, 822, 33064, 581, 683, 33207, 659, 21838, 14187, 235269, 3965, 907, 28265, 13579, 102427, 7495, 16136, 597, 129857, 14971, 26037, 235265, 109, 235310, 235265, 10453, 9233, 581, 39463, 235292, 129228, 37885, 2032, 3267, 121979, 581, 87404, 907, 17227, 235269, 12729, 907, 875, 14341, 50043, 1749, 907, 4176, 81835, 1301, 2575, 16432, 581, 24089, 597, 10444, 23803, 235265, 109, 1776, 37538, 476, 683, 61379, 235269, 13376, 875, 14341, 195557, 6048, 1395, 11226, 33679, 683, 33207, 15255, 581, 7467, 235265, 2132, 10609, 19801, 581, 1749, 60440, 493, 748, 163242, 875, 8884, 1301, 683, 34830, 581, 1778, 37885, 235269, 3965, 907, 1437, 80168, 16601, 4158, 235265, 10597, 20378, 235269, 875, 14341, 14494, 659, 13691, 907, 683, 16601, 12677, 77812, 7492, 1369, 842, 114306, 1301, 28265, 37885, 235269, 12729, 907, 34871, 173396, 581, 50043, 748, 10609, 67996, 1301, 683, 62320, 907, 156744, 114353, 235265], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'conversations': [{'from': 'human', 'value': 'I have been trying to learn japanese, but I keep getting the particles mixed up. In particular, I keep getting \"wa\", \"o\" (or \"wo\", I guess), and \"no\" mixed up. Could you remind me again how each of these are used, and give me some examples?'}, {'from': 'gpt', 'value': 'Japanese particles can be confusing at first, but with practice, you\\'ll get the hang of it. Let me clarify the usage of \"wa\" (は), \"o\" (を), and \"no\" (の) for you.\\n\\nLet\\'s start with \"wa\" (は):\\n\\n1. \"wa\" (は): This is the topic marker. It indicates the main topic of the sentence, which is often the subject. It helps to set the focus of the sentence.\\n\\nExample:\\n\\n* わたしは りんごが すきです。(Watashi wa ringo ga suki desu) -> I like apples. (Here, \"I\" (わたし) is the main topic of the sentence.)\\n\\n\"o\" (を):\\n\\n2. \"o\" (を): This particle is used to mark the direct object of an action, showing what the action is being performed on.\\n\\nExample:\\n\\n* ほんを よみます。(Hon o yomimasu) -> I read a book. (Here, \"book\" (ほん) is the direct object of the action \"to read\" (よみます).)\\n\\n\"no\" (の):\\n\\n3. \"no\" (の): This particle has multiple uses, but the most common one is to indicate possession or to link nouns together, similar to \"of\" or \" \\'s\" in English.\\n\\nExample:\\n\\n* わたしの くるま (Watashi no kuruma) -> My car (Here, \"の\" shows possession, indicating the car belongs to \"me\" (わたし).)\\n\\nI hope this helps! Here\\'s a sentence using all three particles:\\n\\n* たなかさんは ねこの えさを あげます。(Tanaka-san wa neko no esa o agemasu) -> Tanaka-san feeds the cat food. (Here, \"Tanaka-san\" is the topic, \"cat food\" (ねこのえさ) is the direct object, and \"no\" is used to link \"cat\" and \"food\".)'}, {'from': 'human', 'value': 'Now write a poem with the above sentences in Japanese and English.'}, {'from': 'gpt', 'value': \"Here's a poem I came up with using the sentences we discussed earlier, in both Japanese and English:\\n\\nわたしはりんごがすきです。\\nI like apples.\\nほんをよみます。\\nI read a book.\\nわたしのくるま。\\nMy car.\\nたなかさんはねこのえさをあげます。\\nTanaka-san feeds the cat food.\\n\\nりんごは好きだ。\\n本を読む。\\n私の車。\\n田中さんは猫のエサをあげる。\\n\\nApples, I adore.\\nA book, I explore.\\nMy car, I adore.\\nTanaka-san feeds the cat food, for sure.\\n\\nI hope you enjoy this poem!\"}], 'input_ids': [2, 17877, 235292, 590, 791, 1125, 5326, 577, 3918, 46344, 235269, 901, 590, 2745, 4225, 573, 16071, 10616, 908, 235265, 878, 3666, 235269, 590, 2745, 4225, 664, 2534, 824, 664, 235253, 235281, 591, 483, 664, 1100, 824, 590, 7464, 823, 578, 664, 956, 235281, 10616, 908, 235265, 20337, 692, 30903, 682, 1653, 1368, 1853, 576, 1450, 708, 1671, 235269, 578, 2734, 682, 1009, 8944, 235336, 583, 1071, 235292, 11430, 16071, 798, 614, 39809, 696, 1370, 235269, 901, 675, 6016, 235269, 692, 235303, 529, 947, 573, 12009, 576, 665, 235265, 4371, 682, 40405, 573, 14335, 576, 664, 2534, 235281, 591, 235418, 823, 664, 235253, 235281, 591, 235432, 823, 578, 664, 956, 235281, 591, 235372, 235275, 604, 692, 235265, 109, 5331, 235303, 235256, 2238, 675, 664, 2534, 235281, 591, 235418, 1245, 109, 235274, 235265, 664, 2534, 235281, 591, 235418, 1245, 1417, 603, 573, 11381, 24416, 235265, 1165, 14939, 573, 1872, 11381, 576, 573, 13060, 235269, 948, 603, 3695, 573, 5091, 235265, 1165, 7154, 577, 1142, 573, 6045, 576, 573, 13060, 235265, 109, 8036, 235292, 109, 235287, 235248, 205216, 63596, 180299, 235425, 50238, 235520, 2297, 56969, 21964, 4518, 5278, 7408, 235253, 8550, 742, 1661, 217720, 235275, 3978, 590, 1154, 34188, 235265, 591, 4858, 235269, 664, 235285, 235281, 591, 85284, 235275, 603, 573, 1872, 11381, 576, 573, 13060, 2330, 109, 235281, 235253, 235281, 591, 235432, 1245, 109, 235284, 235265, 664, 235253, 235281, 591, 235432, 1245, 1417, 19711, 603, 1671, 577, 2110, 573, 3280, 4018, 576, 671, 3105, 235269, 8532, 1212, 573, 3105, 603, 1855, 8261, 611, 235265, 109, 8036, 235292, 109, 235287, 235248, 43802, 235432, 67194, 99906, 56969, 16192, 493, 597, 537, 23771, 235261, 235275, 3978, 590, 1682, 476, 2870, 235265, 591, 4858, 235269, 664, 2545, 235281, 591, 43802, 235275, 603, 573, 3280, 4018, 576, 573, 3105, 664, 511, 1682, 235281, 591, 235590, 99906, 164948, 109, 235281, 956, 235281, 591, 235372, 1245, 109, 235304, 235265, 664, 956, 235281, 591, 235372, 1245, 1417, 19711, 919, 6733, 7177, 235269, 901, 573, 1546, 3818, 974, 603, 577, 12181, 17680, 689, 577, 3615, 83293, 3584, 235269, 3968, 577, 664, 559, 235281, 689, 664, 777, 235256, 235281, 575, 4645, 235265, 109, 8036, 235292, 109, 235287, 235248, 56012, 58785, 196581, 235422, 591, 21964, 4518, 793, 10958, 6704, 235275, 3978, 2961, 1226, 591, 4858, 235269, 664, 235372, 235281, 4918, 17680, 235269, 24830, 573, 1226, 17239, 577, 664, 504, 235281, 591, 85284, 164948, 109, 235285, 4077, 736, 7154, 235341, 5698, 235303, 235256, 476, 13060, 2177, 832, 2149, 16071, 235292, 109, 235287, 9033, 74317, 42207, 56697, 6586, 91074, 63892, 25758, 236359, 2286, 56969, 28919, 6847, 235290, 14992, 5278, 146746, 793, 19242, 493, 3911, 2616, 235261, 235275, 3978, 102746, 235290, 14992, 46200, 573, 4401, 2960, 235265, 591, 4858, 235269, 664, 28919, 6847, 235290, 14992, 235281, 603, 573, 11381, 235269, 664, 4991, 2960, 235281, 591, 235850, 6586, 235685, 235512, 235275, 603, 573, 3280, 4018, 235269, 578, 664, 956, 235281, 603, 1671, 577, 3615, 664, 4991, 235281, 578, 664, 11804, 235281, 2330, 3515, 235292, 4414, 5598, 476, 19592, 675, 573, 3131, 26099, 575, 11430, 578, 4645, 235265, 583, 1071, 235292, 5698, 235303, 235256, 476, 19592, 590, 3392, 908], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig,PeftModel\n\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:11:56.574881Z","iopub.execute_input":"2024-07-19T21:11:56.575253Z","iopub.status.idle":"2024-07-19T21:11:56.580204Z","shell.execute_reply.started":"2024-07-19T21:11:56.575223Z","shell.execute_reply":"2024-07-19T21:11:56.579280Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Apply PEFT configuration to the model\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=5,\n    warmup_steps=2,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=1,\n    evaluation_strategy=\"steps\",       # Evaluate every X steps\n    eval_steps=50,                    # Evaluate every 100 steps\n    save_strategy=\"steps\",             # Save every X steps\n    save_steps=50,                    # Save every 100 steps\n    save_total_limit=2,                # Limit the total saved checkpoints\n    load_best_model_at_end=True,       # Load the best model at the end of training\n    metric_for_best_model=\"eval_loss\", # Metric to decide the best model\n    output_dir=\"outputs\",\n    optim=\"paged_adamw_8bit\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:11:47.684533Z","iopub.execute_input":"2024-07-19T21:11:47.684914Z","iopub.status.idle":"2024-07-19T21:11:47.718291Z","shell.execute_reply.started":"2024-07-19T21:11:47.684886Z","shell.execute_reply":"2024-07-19T21:11:47.717349Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from trl import SFTConfig, SFTTrainer\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:11:50.769291Z","iopub.execute_input":"2024-07-19T21:11:50.769656Z","iopub.status.idle":"2024-07-19T21:11:50.802883Z","shell.execute_reply.started":"2024-07-19T21:11:50.769622Z","shell.execute_reply":"2024-07-19T21:11:50.802186Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Trainer\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=dataset_split['train'],\n    eval_dataset=dataset_split['validation'],  # Set evaluation dataset\n    args=training_args,\n    peft_config=lora_config,\n    max_seq_length=512  # Adjust based on your requirements and GPU capacity\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:25:54.368039Z","iopub.execute_input":"2024-07-19T21:25:54.368766Z","iopub.status.idle":"2024-07-19T21:25:55.418745Z","shell.execute_reply.started":"2024-07-19T21:25:54.368733Z","shell.execute_reply":"2024-07-19T21:25:55.417589Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:25:57.132609Z","iopub.execute_input":"2024-07-19T21:25:57.133257Z","iopub.status.idle":"2024-07-19T21:25:57.206527Z","shell.execute_reply.started":"2024-07-19T21:25:57.133223Z","shell.execute_reply":"2024-07-19T21:25:57.205542Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:25:59.790899Z","iopub.execute_input":"2024-07-19T21:25:59.791537Z","iopub.status.idle":"2024-07-19T21:35:59.486691Z","shell.execute_reply.started":"2024-07-19T21:25:59.791503Z","shell.execute_reply":"2024-07-19T21:35:59.485748Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1075' max='1075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1075/1075 09:58]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"metrics","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:36:36.129599Z","iopub.execute_input":"2024-07-19T21:36:36.130294Z","iopub.status.idle":"2024-07-19T21:36:36.137938Z","shell.execute_reply.started":"2024-07-19T21:36:36.130258Z","shell.execute_reply":"2024-07-19T21:36:36.136824Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 1.6639602184295654,\n 'eval_runtime': 599.4813,\n 'eval_samples_per_second': 1.793,\n 'eval_steps_per_second': 1.793}"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:23:08.568797Z","iopub.execute_input":"2024-07-19T21:23:08.569383Z","iopub.status.idle":"2024-07-19T21:23:08.578817Z","shell.execute_reply.started":"2024-07-19T21:23:08.569350Z","shell.execute_reply":"2024-07-19T21:23:08.577268Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"download_file('/kaggle/working/outputs/checkpoint-160', 'out')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:23:56.854971Z","iopub.execute_input":"2024-07-19T21:23:56.855843Z","iopub.status.idle":"2024-07-19T21:24:00.949299Z","shell.execute_reply.started":"2024-07-19T21:23:56.855808Z","shell.execute_reply":"2024-07-19T21:24:00.948229Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/out.zip","text/html":"<a href='out.zip' target='_blank'>out.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"outputs\")\ntrainer.save_tokenizer(\"outputs\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train(resume_from_checkpoint=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = trainer.evaluate()\n# Log metrics to Weights & Biases\nwandb.log(metrics)\n# Finish Weights & Biases run\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Save the model locally at the end of training\nmodel_save_path = \"outputs/final_model\"\ntrainer.save_model(model_save_path)\ntokenizer.save_pretrained(model_save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Upload the model to Hugging Face Hub\npeft_model.push_to_hub(\"kanak8278/gemma-2b-oasst2-01\", use_auth_token=True)\ntokenizer.push_to_hub(\"kanak8278/gemma-2b-oasst2-01\", use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:40:18.600669Z","iopub.execute_input":"2024-07-19T21:40:18.601311Z","iopub.status.idle":"2024-07-19T21:40:28.485670Z","shell.execute_reply.started":"2024-07-19T21:40:18.601282Z","shell.execute_reply":"2024-07-19T21:40:28.484537Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:875: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"754ebb3230f24d5596014da77c92cc2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/39.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5e2eea82ed743028dee4d0ae95e8f6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4717c843d6d4be0a5fe178435d1c9f1"}},"metadata":{}},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/kanak8278/gemma-2b-oasst2-01/commit/aa67c4d82fdaab888f26a7ab30c269707c7f708c', commit_message='Upload tokenizer', commit_description='', oid='aa67c4d82fdaab888f26a7ab30c269707c7f708c', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"eval_dataset = dataset_split[\"validation\"]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:41:06.359974Z","iopub.execute_input":"2024-07-19T21:41:06.360733Z","iopub.status.idle":"2024-07-19T21:41:06.366003Z","shell.execute_reply.started":"2024-07-19T21:41:06.360700Z","shell.execute_reply":"2024-07-19T21:41:06.365030Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"eval_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:41:10.124982Z","iopub.execute_input":"2024-07-19T21:41:10.125815Z","iopub.status.idle":"2024-07-19T21:41:10.132709Z","shell.execute_reply.started":"2024-07-19T21:41:10.125776Z","shell.execute_reply":"2024-07-19T21:41:10.131544Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['conversations', 'input_ids', 'attention_mask'],\n    num_rows: 1075\n})"},"metadata":{}}]},{"cell_type":"code","source":"model.to('cuda:0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to generate predictions\ndef generate_predictions(input_ids, attention_mask):\n    with torch.no_grad():\n        outputs = peft_model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=50)\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n# Apply the prediction function\npredictions = []\n\n# Iterate over the dataset in batches\nfor batch in eval_dataset:\n    # Convert batch to tensors\n    input_ids = torch.tensor(batch['input_ids'])\n    attention_mask = torch.tensor(batch['attention_mask'])\n    \n    # Generate predictions for the batch\n    batch_predictions = generate_predictions(input_ids, attention_mask)\n    predictions.extend(batch_predictions)\n    \n    # Optional: break if you only want to process a subset\n    break\n\n# Print out some of the predictions\nfor i in range(5):  # Adjust as needed\n    print(f\"Prediction {i+1}: {predictions[i]}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:41:14.404972Z","iopub.execute_input":"2024-07-19T21:41:14.405349Z","iopub.status.idle":"2024-07-19T21:41:15.194248Z","shell.execute_reply.started":"2024-07-19T21:41:14.405319Z","shell.execute_reply":"2024-07-19T21:41:15.191335Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Generate predictions for the batch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(batch_predictions)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Optional: break if you only want to process a subset\u001b[39;00m\n","Cell \u001b[0;32mIn[49], line 4\u001b[0m, in \u001b[0;36mgenerate_predictions\u001b[0;34m(input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_predictions\u001b[39m(input_ids, attention_mask):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpeft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1491\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1490\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1491\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1493\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1911\u001b[0m     )\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2648\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2644\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[1;32m   2646\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m   2647\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 2648\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2650\u001b[0m     \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m   2651\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   2652\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2653\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2654\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   2655\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2656\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:1192\u001b[0m, in \u001b[0;36mGemmaForCausalLM.prepare_inputs_for_generation\u001b[0;34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m cache_length \u001b[38;5;241m=\u001b[39m past_length \u001b[38;5;28;01mif\u001b[39;00m max_cache_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmin(max_cache_length, past_length)\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# Keep only the unprocessed tokens:\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as input)\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m   1193\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, \u001b[38;5;241m-\u001b[39m(attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m past_length) :]\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;66;03m# 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# input_ids based on the past_length.\u001b[39;00m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"],"ename":"IndexError","evalue":"tuple index out of range","output_type":"error"}]},{"cell_type":"code","source":"eval_dataset['conversations'][1]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T21:49:10.318541Z","iopub.execute_input":"2024-07-19T21:49:10.319331Z","iopub.status.idle":"2024-07-19T21:49:10.372221Z","shell.execute_reply.started":"2024-07-19T21:49:10.319299Z","shell.execute_reply":"2024-07-19T21:49:10.371160Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"[{'from': 'human',\n  'value': 'Is an ergonomic mouse really worth how weird it looks?'},\n {'from': 'gpt',\n  'value': \"Yes, an ergonomic mouse can be worth the unique design for many people, especially those who spend extended periods of time using a computer mouse. Here are some pros and cons to consider:\\n\\nPros of an ergonomic mouse:\\n- Improved comfort: Ergonomic mice are designed to fit the natural shape and movement of your hand, which can reduce strain on your wrist, forearm, and shoulder, and potentially minimize the risk of developing musculoskeletal disorders like carpal tunnel syndrome.\\n- Enhanced productivity: A comfortable mouse can allow for more precise and efficient mouse movements, which can boost productivity, especially for tasks that require fine control, such as design work or detailed photo editing.\\n- Customizable features: Many ergonomic mice come with customizable buttons and settings, allowing you to tailor the mouse to your preferences and workflow.\\n- Long-term health benefits: Using an ergonomic mouse may help prevent or reduce the risk of developing repetitive strain injuries (RSIs) associated with prolonged computer use.\\n\\nCons of an ergonomic mouse:\\n- Learning curve: An ergonomic mouse may require some time to get used to, especially if you have been using a traditional mouse for a long time.\\n- Cost: Ergonomic mice can be more expensive than traditional mice, although prices can vary widely depending on the brand, model, and features.\\n- Aesthetic preferences: Some people may find the appearance of ergonomic mice to be unconventional or less visually appealing compared to traditional mice.\\n\\nUltimately, whether an ergonomic mouse is worth the unique design depends on your personal preferences, needs, and budget. If you experience discomfort or pain from prolonged mouse use, an ergonomic mouse could be a worthwhile investment to potentially improve your comfort, productivity, and long-term health. It's also important to consider factors like customization options, user reviews, and trying out different models to find the one that best fits your hand shape and usage habits.\"}]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}